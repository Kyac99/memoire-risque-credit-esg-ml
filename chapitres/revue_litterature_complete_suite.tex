\\section{L'essor du Machine Learning dans la gestion du risque de crédit}\n\n\\subsection{Modèles supervisés : Random Forest, XGBoost, réseaux de neurones}\n\nL'application des techniques de machine learning à la modélisation du risque de crédit a connu un essor considérable ces dernières années, offrant des alternatives aux approches statistiques traditionnelles. Les modèles supervisés, en particulier, ont démontré une capacité remarquable à capturer des relations complexes et non linéaires entre les variables explicatives et le risque de défaut.\n\nLes algorithmes de Random Forest, introduits par \\citet{breiman2001}, constituent l'une des approches les plus utilisées dans ce domaine. Ces modèles d'ensemble combinent de multiples arbres de décision entraînés sur des sous-échantillons aléatoires des données et des sous-ensembles aléatoires de variables, réduisant ainsi le risque de surapprentissage. Formellement, la prédiction d'un modèle Random Forest peut être exprimée comme :\n\n\\begin{equation}\n\\hat{y} = \\frac{1}{B} \\sum_{b=1}^{B} T_b(X)\n\\end{equation}\n\noù B est le nombre d'arbres, $T_b$ représente le b-ième arbre, et X le vecteur de variables explicatives.\n\nChaque arbre individuel est construit selon un algorithme récursif de partitionnement binaire qui maximise la réduction d'impureté à chaque nœud, typiquement mesurée par l'indice de Gini pour les problèmes de classification :\n\n\\begin{equation}\nG = \\sum_{i=1}^{C} p_i(1-p_i) = 1 - \\sum_{i=1}^{C} p_i^2\n\\end{equation}\n\noù $p_i$ représente la proportion d'observations appartenant à la classe $i$ dans un nœud donné, et $C$ le nombre total de classes.\n\n\\citet{brown2012} ont comparé les performances de différentes techniques de machine learning, dont Random Forest, pour la prédiction du risque de crédit sur des ensembles de données déséquilibrés, caractéristiques des portefeuilles de crédit où les défauts sont rares. Leur étude a démontré que Random Forest surpassait systématiquement les méthodes statistiques traditionnelles, notamment les modèles logit et probit, ainsi que d'autres algorithmes de machine learning comme les SVM (Support Vector Machines), particulièrement dans les cas d'asymétrie prononcée entre les classes.\n\nXGBoost (eXtreme Gradient Boosting), développé par \\citet{chen2016}, représente une évolution majeure des algorithmes de boosting pour les arbres de décision. Contrairement à Random Forest qui construit des arbres indépendants en parallèle, XGBoost adopte une approche séquentielle où chaque nouvel arbre est optimisé pour corriger les erreurs des arbres précédents. La fonction objective de XGBoost combine un terme de perte et un terme de régularisation :\n\n\\begin{equation}\n\\mathcal{L} = \\sum_{i=1}^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)\n\\end{equation}\n\noù $l$ est la fonction de perte, $\\hat{y}_i$ la prédiction pour l'observation i, $\\Omega$ le terme de régularisation, et $f_k$ représente le k-ième arbre du modèle.\n\nLa fonction de perte $l$ peut prendre différentes formes selon la nature du problème. Pour la classification binaire du risque de crédit, la fonction de perte logistique est couramment utilisée :\n\n\\begin{equation}\nl(y_i, \\hat{y}_i) = y_i \\ln(1 + e^{-\\hat{y}_i}) + (1 - y_i) \\ln(1 + e^{\\hat{y}_i})\n\\end{equation}\n\nLe terme de régularisation $\\Omega(f)$ contrôle la complexité des arbres pour prévenir le surapprentissage :\n\n\\begin{equation}\n\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2\n\\end{equation}\n\noù $T$ représente le nombre de feuilles dans l'arbre, $w_j$ les scores associés à chaque feuille, $\\gamma$ et $\\lambda$ étant des hyperparamètres contrôlant respectivement la pénalité pour chaque feuille additionnelle et la régularisation L2 sur les poids.\n\nDans une étude comparative extensive, \\citet{xia2017} ont évalué la performance de XGBoost contre plusieurs autres techniques pour la prédiction du risque de crédit sur 10 ensembles de données de référence. Leurs résultats ont révélé que XGBoost surpassait systématiquement les autres approches en termes de précision et d'aire sous la courbe ROC (AUC), tout en offrant une meilleure stabilité face aux variations dans les données d'entraînement.\n\nLes réseaux de neurones artificiels (ANN) constituent une troisième catégorie majeure de modèles supervisés appliqués au risque de crédit. Dans leur forme la plus simple, les réseaux de neurones multicouches transforment un vecteur d'entrée X en une prédiction y à travers une série de transformations non linéaires :\n\n\\begin{align}\nh^{(1)} &= \\sigma(W^{(1)}X + b^{(1)}) \\\\\nh^{(2)} &= \\sigma(W^{(2)}h^{(1)} + b^{(2)}) \\\\\n\\vdots \\\\\ny &= \\sigma(W^{(L)}h^{(L-1)} + b^{(L)})\n\\end{align}\n\noù $h^{(l)}$ représente la l-ième couche cachée, $W^{(l)}$ et $b^{(l)}$ les paramètres associés, et $\\sigma$ une fonction d'activation non linéaire comme la fonction ReLU (Rectified Linear Unit) : $\\sigma(x) = max(0, x)$.\n\nPour les problèmes de classification binaire comme la prédiction du défaut, la couche de sortie utilise typiquement une fonction d'activation sigmoïde pour produire une probabilité :\n\n\\begin{equation}\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\end{equation}\n\nL'apprentissage du réseau s'effectue par descente de gradient sur une fonction de perte, généralement l'entropie croisée binaire pour les problèmes de risque de crédit :\n\n\\begin{equation}\nJ(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\n\\end{equation}\n\noù $\\theta$ représente l'ensemble des paramètres du réseau, $m$ le nombre d'exemples d'entraînement, $y_i$ les classes réelles et $\\hat{y}_i$ les probabilités prédites.\n\n\\citet{kvamme2018} ont proposé une architecture de réseau neuronal profond spécifiquement conçue pour la modélisation du risque de crédit, intégrant non seulement la prédiction de la probabilité de défaut mais aussi l'estimation de la distribution temporelle des défauts. Leur modèle, basé sur des réseaux de neurones récurrents avec portes (LSTM - Long Short-Term Memory), a démontré une capacité supérieure à capturer les dépendances temporelles dans les données de crédit par rapport aux approches traditionnelles de modélisation de survie.\n\n\\begin{align}\nh_t &= LSTM(x_t, h_{t-1})\\\\\nS(t) &= \\sigma(W_s h_t + b_s)\n\\end{align}\n\noù $S(t)$ représente la fonction de survie estimée, indiquant la probabilité de non-défaut jusqu'au temps t.\n\nL'unité LSTM est définie par les équations suivantes, permettant de capturer efficacement les dépendances à long terme dans les séquences temporelles :\n\n\\begin{align}\nf_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\quad \\text{(porte d'oubli)} \\\\\ni_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\quad \\text{(porte d'entrée)} \\\\\no_t &= \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\quad \\text{(porte de sortie)} \\\\\n\\tilde{C}_t &= \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\quad \\text{(candidat à la mémoire cellulaire)} \\\\\nC_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\quad \\text{(mise à jour de la mémoire cellulaire)} \\\\\nh_t &= o_t \\odot \\tanh(C_t) \\quad \\text{(état caché de sortie)}\n\\end{align}\n\noù $\\odot$ représente le produit de Hadamard (élément par élément).\n\nPlus récemment, \\citet{babaev2019} ont exploré l'utilisation de techniques d'apprentissage profond avancées, notamment les modèles d'attention, pour la prédiction du risque de crédit à partir de séquences d'événements transactionnels. Leur approche permet de pondérer dynamiquement l'importance de différentes transactions dans l'historique d'un client, offrant ainsi une capacité de modélisation plus fine des comportements risqués :\n\n\\begin{align}\n\\alpha_i &= \\frac{\\exp(e_i^T W_{\\alpha} h)}{\\sum_j \\exp(e_j^T W_{\\alpha} h)}\\\\\nc &= \\sum_i \\alpha_i e_i\n\\end{align}\n\noù $\\alpha_i$ représente le poids d'attention attribué à l'événement i, $e_i$ son encodage vectoriel, et c le vecteur de contexte résultant, utilisé pour la prédiction finale.\n\nLe mécanisme d'attention multi-têtes généralise cette approche en permettant au modèle de se concentrer simultanément sur différents aspects des données d'entrée :\n\n\\begin{align}\nhead_i &= Attention(QW_i^Q, KW_i^K, VW_i^V) \\\\\nMultiHead(Q, K, V) &= Concat(head_1, \\ldots, head_h)W^O\n\\end{align}\n\noù $Q$, $K$, et $V$ représentent respectivement les matrices de requête, clé et valeur, et les matrices $W_i^Q$, $W_i^K$, $W_i^V$ et $W^O$ sont des paramètres appris.\n\nDans une perspective comparative globale, \\citet{lessmann2015} ont conduit une analyse exhaustive des méthodes de machine learning pour le scoring de crédit, évaluant 41 classifieurs différents sur plusieurs ensembles de données de référence. Leurs résultats ont confirmé la supériorité des méthodes d'ensemble comme Random Forest et Gradient Boosting, tout en soulignant que la combinaison de plusieurs modèles hétérogènes (stacking) pouvait conduire à des performances encore supérieures. Ils ont également mis en évidence l'importance d'une évaluation appropriée des modèles, recommandant l'utilisation de métriques adaptées aux problématiques de crédit comme le H-measure et l'AUC ajustée aux coûts (Cost-AUC).\n\n\\subsection{Modèles non supervisés : clustering et réduction de dimension}\n\nLes techniques d'apprentissage non supervisé, bien que moins directement appliquées à la prédiction du risque de crédit que les modèles supervisés, jouent néanmoins un rôle important dans la préparation des données, la détection d'anomalies et la segmentation des portefeuilles. Ces approches permettent d'extraire des structures et des patterns inhérents aux données sans nécessiter d'étiquettes préalables.\n\nLes algorithmes de clustering visent à regrouper les observations similaires, permettant ainsi d'identifier des segments homogènes au sein d'un portefeuille de crédit. Parmi ces techniques, l'algorithme K-means demeure l'un des plus utilisés. Il cherche à minimiser la variance intra-cluster en partitionnant les données en K groupes :\n\n\\begin{equation}\n\\min_{C} \\sum_{k=1}^K \\sum_{i \\in C_k} ||x_i - \\mu_k||^2\n\\end{equation}\n\noù $C_k$ représente le k-ième cluster, $x_i$ les observations, et $\\mu_k$ le centroïde du cluster k.\n\nL'algorithme K-means procède de manière itérative selon les étapes suivantes :\n\n\\begin{align}\n\\text{1. Initialisation :} & \\text{ Choisir aléatoirement K points comme centroïdes initiaux } \\mu_1, \\mu_2, \\ldots, \\mu_K \\\\\n\\text{2. Attribution :} & \\text{ Assigner chaque point au cluster du centroïde le plus proche} \\\\\n& C_k^{(t)} = \\{x_i : ||x_i - \\mu_k^{(t)}||^2 \\leq ||x_i - \\mu_j^{(t)}||^2 \\text{ pour tout } j = 1, \\ldots, K\\} \\\\\n\\text{3. Mise à jour :} & \\text{ Recalculer les centroïdes comme la moyenne des points dans chaque cluster} \\\\\n& \\mu_k^{(t+1)} = \\frac{1}{|C_k^{(t)}|} \\sum_{x_i \\in C_k^{(t)}} x_i \\\\\n\\text{4. Itération :} & \\text{ Répéter les étapes 2 et 3 jusqu'à convergence}\n\\end{align}\n\nLa convergence est atteinte lorsque les assignations ne changent plus ou lorsque la variation des centroïdes devient inférieure à un seuil prédéfini.\n\n\\citet{harris2015} a appliqué cette approche à la segmentation des emprunteurs obligataires, démontrant que l'identification de segments homogènes permettait d'améliorer significativement la précision des modèles de risque spécifiques à chaque segment par rapport à un modèle global. Son étude a également souligné l'importance d'une sélection appropriée du nombre de clusters, suggérant l'utilisation de critères comme l'indice de silhouette ou la méthode du coude (elbow method).\n\nL'indice de silhouette pour une observation $i$ est défini comme :\n\n\\begin{equation}\ns(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n\\end{equation}\n\noù $a(i)$ est la distance moyenne entre l'observation $i$ et toutes les autres observations dans le même cluster, et $b(i)$ est la distance moyenne minimale entre $i$ et toutes les observations dans un autre cluster. Un score proche de 1 indique que l'observation est bien placée dans son cluster.\n\nDes approches plus sophistiquées comme le clustering hiérarchique agglomératif offrent une flexibilité supplémentaire en construisant une hiérarchie de clusters sans nécessiter la spécification préalable de leur nombre. Comme le décrit \\citet{aggarwal2014}, cette méthode procède par fusion successive des clusters les plus similaires, jusqu'à n'en former plus qu'un seul :\n\n\\begin{align}\nd(C_i, C_j) &= \\min_{x \\in C_i, y \\in C_j} d(x, y) \\quad \\text{(single linkage)}\\\\\nd(C_i, C_j) &= \\max_{x \\in C_i, y \\in C_j} d(x, y) \\quad \\text{(complete linkage)}\\\\\nd(C_i, C_j) &= \\frac{1}{|C_i||C_j|} \\sum_{x \\in C_i} \\sum_{y \\in C_j} d(x, y) \\quad \\text{(average linkage)}\n\\end{align}\n\noù $d(C_i, C_j)$ représente la distance entre les clusters $C_i$ et $C_j$, calculée selon différentes définitions (single, complete ou average linkage).\n\nL'algorithme général du clustering hiérarchique agglomératif peut être décrit comme suit :\n\n\\begin{align}\n\\text{1. Initialisation :} & \\text{ Chaque observation forme son propre cluster} \\\\\n\\text{2. Fusion :} & \\text{ À chaque étape, fusionner les deux clusters les plus proches} \\\\\n& (C_i, C_j) = \\arg\\min_{C_i, C_j} d(C_i, C_j) \\\\\n\\text{3. Mise à jour :} & \\text{ Recalculer les distances entre le nouveau cluster et tous les autres} \\\\\n\\text{4. Itération :} & \\text{ Répéter les étapes 2 et 3 jusqu'à n'avoir plus qu'un seul cluster}\n\\end{align}\n\nLe résultat de ce processus est généralement représenté sous forme de dendrogramme, permettant de visualiser la hiérarchie des clusters et de choisir un niveau de coupure approprié selon les besoins de l'analyse.\n\nDans le domaine spécifique du risque de crédit obligataire, \\citet{poon2019} ont appliqué ces techniques à l'identification de groupes d'émetteurs présentant des profils de risque similaires. Leur approche, combinant clustering hiérarchique et analyse en composantes principales, a permis de révéler des structures de risque latentes non apparentes dans les classifications sectorielles traditionnelles, offrant ainsi des opportunités de diversification plus efficaces pour les gestionnaires de portefeuilles obligataires.\n\nLes méthodes de réduction de dimension constituent une autre catégorie importante d'approches non supervisées, particulièrement utiles pour traiter la haute dimensionnalité des données financières et ESG. L'Analyse en Composantes Principales (ACP), introduite par \\citet{pearson1901} et développée par \\citet{hotelling1933}, reste l'une des techniques les plus utilisées. Elle vise à projeter les données dans un espace de dimension réduite tout en maximisant la variance expliquée :\n\n\\begin{equation}\n\\max_{w} \\frac{w^T X^T X w}{w^T w}\n\\end{equation}\n\nsous contrainte $w^T w = 1$, où X représente la matrice de données centrées, et w les vecteurs propres de la matrice de covariance $X^T X$.\n\nMathématiquement, l'ACP consiste à trouver les vecteurs propres et valeurs propres de la matrice de covariance :\n\n\\begin{equation}\nC = \\frac{1}{n} X^T X\n\\end{equation}\n\noù $n$ est le nombre d'observations et $X$ la matrice de données centrées.\n\nLes composantes principales sont alors les vecteurs propres $v_1, v_2, \\ldots, v_d$ correspondant aux $d$ plus grandes valeurs propres $\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_d$. La projection des données originales sur ces composantes principales s'obtient par :\n\n\\begin{equation}\nZ = X V\n\\end{equation}\n\noù $V = [v_1, v_2, \\ldots, v_d]$ est la matrice des vecteurs propres sélectionnés.\n\nLa proportion de variance expliquée par les $k$ premières composantes principales est donnée par :\n\n\\begin{equation}\n\\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i}\n\\end{equation}\n\nAppliquée aux critères ESG, l'ACP permet de réduire la redondance entre les multiples indicateurs et d'identifier les dimensions latentes principales. \\citet{dorfleitner2018} ont utilisé cette approche pour analyser la structure sous-jacente des notations ESG, démontrant que malgré la multiplicité des indicateurs, un nombre relativement restreint de composantes principales (généralement 3 à 5) capturait la majeure partie de la variance. Cette observation suggère une dimensionnalité intrinsèque relativement faible des données ESG, facilitant leur intégration dans les modèles de risque de crédit.\n\nDes méthodes non linéaires de réduction de dimension, comme t-SNE (t-distributed Stochastic Neighbor Embedding) développée par \\citet{vandermaaten2008}, offrent des perspectives complémentaires en préservant les structures locales des données. Contrairement à l'ACP qui se concentre sur la variance globale, t-SNE minimise la divergence de Kullback-Leibler entre les distributions de probabilité dans l'espace original et l'espace réduit :\n\n\\begin{equation}\nKL(P || Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n\\end{equation}\n\noù $p_{ij}$ et $q_{ij}$ représentent respectivement les similitudes entre les points i et j dans l'espace original et l'espace réduit.\n\nLes similitudes dans l'espace original sont définies comme des probabilités conditionnelles :\n\n\\begin{equation}\np_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}\n\\end{equation}\n\noù $\\sigma_i$ est calculé automatiquement selon la densité locale des points (perplexité).\n\nLes similitudes dans l'espace réduit sont définies par une distribution t de Student à un degré de liberté :\n\n\\begin{equation}\nq_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}}\n\\end{equation}\n\nL'algorithme optimise les coordonnées $y_i$ dans l'espace réduit par descente de gradient pour minimiser la divergence KL.\n\nAppliquée aux données de crédit, cette approche permet de visualiser des clusters d'émetteurs similaires qui pourraient ne pas être apparents avec des méthodes linéaires. Comme le démontrent \\citet{greco2019}, cette visualisation peut être particulièrement utile pour identifier des anomalies ou des tendances émergentes dans les profils de risque, complétant ainsi les modèles prédictifs supervisés.\n\nPlus récemment, les autoencodeurs, une classe de réseaux de neurones conçus pour l'apprentissage de représentations efficientes, ont été appliqués avec succès à la réduction de dimension en finance. Un autoencodeur consiste en un encodeur qui compresse les données d'entrée en une représentation latente de dimension réduite, et un décodeur qui tente de reconstruire les données originales à partir de cette représentation :\n\n\\begin{align}\nh &= f_{\\theta}(x) \\quad \\text{(encodage)}\\\\\n\\hat{x} &= g_{\\phi}(h) \\quad \\text{(décodage)}\n\\end{align}\n\nL'objectif est de minimiser l'erreur de reconstruction : $L(x, \\hat{x}) = ||x - \\hat{x}||^2$, tout en contraignant la dimension de h à être significativement inférieure à celle de x.\n\nPlus formellement, un autoencodeur profond avec $L$ couches dans l'encodeur et le décodeur peut être décrit par :\n\n\\begin{align}\nh^{(1)} &= \\sigma(W^{(1)}x + b^{(1)}) \\\\\nh^{(2)} &= \\sigma(W^{(2)}h^{(1)} + b^{(2)}) \\\\\n\\vdots \\\\\nh^{(L)} &= \\sigma(W^{(L)}h^{(L-1)} + b^{(L)}) \\quad \\text{(représentation latente)} \\\\\n\\vdots \\\\\n\\hat{x} &= \\sigma(W^{(2L)}h^{(2L-1)} + b^{(2L)})\n\\end{align}\n\nLes autoencodeurs variationnels (VAE) ajoutent une contrainte supplémentaire en modélisant la représentation latente comme une distribution probabiliste, généralement gaussienne, et en incluant un terme de régularisation basé sur la divergence de Kullback-Leibler :\n\n\\begin{equation}\nL_{VAE}(x, \\hat{x}) = ||x - \\hat{x}||^2 + \\beta \\cdot KL(q_{\\phi}(z|x) || p(z))\n\\end{equation}\n\noù $q_{\\phi}(z|x)$ est la distribution postérieure approximée de la variable latente z conditionnée à x, $p(z)$ est la distribution a priori (généralement $\\mathcal{N}(0, I)$), et $\\beta$ est un hyperparamètre contrôlant l'importance relative des deux termes.\n\nDans leur étude, \\citet{petropoulos2019} ont appliqué des autoencodeurs variationnels à la modélisation du risque de crédit, démontrant leur capacité à extraire des représentations latentes informatives à partir de données financières et extra-financières hétérogènes. Ces représentations ont ensuite été utilisées comme entrées pour des modèles prédictifs supervisés, améliorant significativement leur performance par rapport à l'utilisation directe des variables brutes.\n\n\\subsection{Métriques de performance ESG et leur modélisation mathématique}\n\nL'intégration des critères ESG dans la modélisation du risque de crédit nécessite une formalisation mathématique permettant de quantifier, agréger et analyser ces facteurs extra-financiers. Cette section explore les principales approches de modélisation des métriques ESG et leur incorporation dans les cadres d'évaluation du risque.\n\n\\subsubsection{Formalisation des scores ESG composites}\n\nLes scores ESG agrégés constituent le niveau le plus synthétique d'évaluation de la performance extra-financière. Leur construction implique généralement une combinaison pondérée de multiples indicateurs sous-jacents, pouvant être formalisée comme suit :\n\n\\begin{equation}\nESG_i = \\sum_{j=1}^{n_E} w_j^E \\cdot E_{i,j} + \\sum_{k=1}^{n_S} w_k^S \\cdot S_{i,k} + \\sum_{l=1}^{n_G} w_l^G \\cdot G_{i,l}\n\\end{equation}\n\noù $ESG_i$ représente le score composite de l'entité $i$, $E_{i,j}$, $S_{i,k}$ et $G_{i,l}$ correspondent respectivement aux indicateurs environnementaux, sociaux et de gouvernance, et $w_j^E$, $w_k^S$, $w_l^G$ sont les pondérations associées à chaque indicateur.\n\n\\citet{drempetic2020} ont analysé systématiquement les méthodologies de construction des scores composites par les principaux fournisseurs de données, révélant d'importantes variations dans les schémas de pondération. Leur étude met en évidence trois approches principales :\n\n\\begin{itemize}\n    \\item \\textbf{Pondération égale}, où chaque pilier (E, S, G) reçoit le même poids : $\\sum_j w_j^E = \\sum_k w_k^S = \\sum_l w_l^G = \\frac{1}{3}$\n    \n    \\item \\textbf{Pondération basée sur la matérialité sectorielle}, où les poids varient selon l'industrie pour refléter l'importance relative des différents enjeux : $w_j^E = f(\\text{secteur})$\n    \n    \\item \\textbf{Pondération statistique}, dérivée empiriquement pour maximiser la capacité prédictive par rapport à certains résultats financiers : $\\mathbf{w} = \\arg\\max_\\mathbf{w} \\text{Corr}(\\sum_j w_j \\cdot X_j, Y)$\n\\end{itemize}\n\nLa normalisation sectorielle constitue une dimension supplémentaire importante dans la construction des scores, permettant des comparaisons équitables entre entreprises d'industries différentes. Formellement, elle peut être exprimée comme :\n\n\\begin{equation}\nESG_i^{norm} = \\frac{ESG_i - \\mu_{s(i)}}{\\sigma_{s(i)}}\n\\end{equation}\n\noù $\\mu_{s(i)}$ et $\\sigma_{s(i)}$ représentent respectivement la moyenne et l'écart-type des scores ESG dans le secteur $s(i)$ auquel appartient l'entité $i$.\n\n\\subsubsection{Quantification des risques ESG spécifiques}\n\nAu-delà des scores composites, des modèles plus spécifiques ont été développés pour quantifier certains risques ESG particulièrement pertinents pour l'évaluation du crédit, notamment les risques climatiques.\n\n\\citet{battiston2017} ont proposé un cadre de modélisation pour évaluer l'exposition des actifs financiers aux risques de transition climatique. Leur approche, basée sur l'analyse des réseaux financiers, quantifie la vulnérabilité des portefeuilles aux chocs climatiques à travers des matrices de dépendance sectorielle :\n\n\\begin{equation}\nV_i = \\sum_{j=1}^S A_{i,j} \\cdot TS_j\n\\end{equation}\n\noù $V_i$ représente la vulnérabilité de l'actif $i$ aux risques de transition, $A_{i,j}$ la proportion de l'exposition de $i$ au secteur $j$, et $TS_j$ le score de sensibilité à la transition du secteur $j$.\n\nLe score de sensibilité à la transition $TS_j$ peut lui-même être modélisé comme une fonction de multiples facteurs :\n\n\\begin{equation}\nTS_j = f(IC_j, RP_j, TA_j, MS_j)\n\\end{equation}\n\noù $IC_j$ représente l'intensité carbone du secteur, $RP_j$ son exposition aux risques politiques et réglementaires, $TA_j$ sa capacité d'adaptation technologique, et $MS_j$ sa sensibilité aux changements de préférences du marché.\n\nConcernant les risques physiques climatiques, \\citet{dietz2018} ont développé un modèle d'évaluation intégrant les dommages climatiques dans la valorisation des actifs financiers. Leur approche évalue la valeur actualisée des pertes attendues dues aux événements climatiques :\n\n\\begin{equation}\nCVaR_\\alpha(i) = E[L_i | L_i > VaR_\\alpha(L_i)]\n\\end{equation}\n\noù $CVaR_\\alpha(i)$ représente la valeur à risque conditionnelle climatique pour l'actif $i$ au niveau de confiance $\\alpha$, $L_i$ les pertes potentielles dues aux impacts climatiques, et $VaR_\\alpha(L_i)$ la valeur à risque correspondante.\n\nLes pertes attendues $L_i$ sont modélisées comme une fonction des projections climatiques et des caractéristiques spécifiques de l'actif :\n\n\\begin{equation}\nL_i = g(\\Delta T, GE_i, AI_i, AC_i)\n\\end{equation}\n\noù $\\Delta T$ représente l'augmentation projetée de la température, $GE_i$ l'exposition géographique de l'actif, $AI_i$ la vulnérabilité de son infrastructure aux aléas climatiques, et $AC_i$ sa capacité d'adaptation.\n\n\\subsubsection{Intégration dans les modèles de risque de crédit}\n\nL'incorporation des métriques ESG dans les modèles de risque de crédit peut suivre plusieurs approches mathématiques. La plus directe consiste à inclure ces facteurs comme variables explicatives supplémentaires dans les modèles existants.\n\nDans un cadre de régression logistique, par exemple, la probabilité de défaut peut être modélisée comme :\n\n\\begin{equation}\nP(\\text{défaut}_i) = \\frac{1}{1 + e^{-(\\alpha + \\boldsymbol{\\beta}' \\mathbf{X}_i + \\boldsymbol{\\gamma}' \\mathbf{ESG}_i)}}\n\\end{equation}\n\noù $\\mathbf{X}_i$ représente le vecteur des variables financières traditionnelles, $\\mathbf{ESG}_i$ le vecteur des métriques ESG, et $\\boldsymbol{\\beta}$ et $\\boldsymbol{\\gamma}$ leurs coefficients respectifs.\n\nUne approche alternative, proposée par \\citet{weber2019}, consiste à développer un modèle à deux étages où les facteurs ESG modulent les paramètres estimés à partir des variables financières traditionnelles :\n\n\\begin{align}\nP(\\text{défaut}_i) &= \\frac{1}{1 + e^{-(\\alpha_i + \\boldsymbol{\\beta}_i' \\mathbf{X}_i)}} \\\\\n\\alpha_i &= \\alpha_0 + \\boldsymbol{\\delta}' \\mathbf{ESG}_i \\\\\n\\boldsymbol{\\beta}_i &= \\boldsymbol{\\beta}_0 + \\mathbf{\\Theta} \\mathbf{ESG}_i\n\\end{align}\n\noù $\\alpha_0$ et $\\boldsymbol{\\beta}_0$ sont les paramètres de base, $\\boldsymbol{\\delta}$ capture l'effet direct des facteurs ESG sur la probabilité de défaut, et la matrice $\\mathbf{\\Theta}$ représente les interactions entre facteurs ESG et variables financières.\n\nDans les modèles structurels de type Merton, l'intégration des facteurs ESG peut se faire via une modulation de la volatilité des actifs ou de la distance au défaut :\n\n\\begin{align}\nDD_i &= \\frac{\\ln(\\frac{V_i}{D_i}) + (r - \\frac{\\sigma_{V,i}^2}{2})T}{\\sigma_{V,i}\\sqrt{T}} \\\\\n\\sigma_{V,i} &= \\sigma_{V,0} \\cdot (1 - \\lambda \\cdot ESG_i)\n\\end{align}\n\noù $DD_i$ représente la distance au défaut, $V_i$ la valeur des actifs, $D_i$ la valeur de la dette, $\\sigma_{V,i}$ la volatilité des actifs, et $\\lambda$ un paramètre capturant l'effet modérateur des performances ESG sur la volatilité.\n\nPour les approches de machine learning plus avancées, \\citet{nemoto2021} ont proposé une architecture de réseau neuronal modulaire spécifiquement conçue pour l'intégration ESG-crédit :\n\n\\begin{align}\nh_{fin} &= f_{fin}(\\mathbf{X}_{fin}) \\\\\nh_{esg} &= f_{esg}(\\mathbf{X}_{esg}) \\\\\nh_{combined} &= \\text{concat}(h_{fin}, h_{esg}) \\\\\ny &= g(h_{combined})\n\\end{align}\n\noù $f_{fin}$ et $f_{esg}$ sont des sous-réseaux spécialisés traitant respectivement les variables financières et ESG, et $g$ est un réseau de fusion produisant la prédiction finale.\n\nCette architecture présente l'avantage de permettre un pré-entraînement séparé des sous-réseaux, facilitant l'incorporation de connaissances spécifiques à chaque domaine et améliorant l'interprétabilité des contributions respectives des facteurs financiers et extra-financiers.\n\n\\subsubsection{Métriques d'évaluation spécifiques aux modèles intégrant l'ESG}\n\nL'évaluation des modèles de risque de crédit intégrant les facteurs ESG nécessite des métriques adaptées, tenant compte de la nature multidimensionnelle de la performance et des objectifs potentiellement complémentaires (financiers et extra-financiers).\n\n\\citet{liang2020} proposent une métrique d'amélioration relative de la performance prédictive due aux facteurs ESG :\n\n\\begin{equation}\nREI_{ESG} = \\frac{Perf(M_{fin+esg}) - Perf(M_{fin})}{Perf(M_{fin})}\n\\end{equation}\n\noù $Perf(M_{fin+esg})$ et $Perf(M_{fin})$ représentent respectivement les performances (mesurées par AUC-ROC, précision, ou autres métriques standard) du modèle complet intégrant les facteurs ESG et du modèle utilisant uniquement les variables financières traditionnelles.\n\nCette métrique peut être décomposée par pilier ESG pour évaluer leur contribution respective :\n\n\\begin{equation}\nREI_E = \\frac{Perf(M_{fin+E}) - Perf(M_{fin})}{Perf(M_{fin})}\n\\end{equation}\n\net de même pour les piliers S et G.\n\nPour évaluer la robustesse temporelle des relations ESG-crédit, \\citet{capelle2021} proposent une métrique de stabilité basée sur la variance des coefficients estimés sur différentes périodes :\n\n\\begin{equation}\nStab(\\gamma_j) = \\frac{1}{\\sigma^2(\\gamma_{j,t})}\n\\end{equation}\n\noù $\\gamma_{j,t}$ représente le coefficient associé au facteur ESG $j$ estimé sur la période $t$.\n\nEnfin, pour les applications pratiques en gestion de portefeuille, des métriques composites ont été développées intégrant à la fois performance financière et ESG. Une exemple notable est le ratio de Sharpe ajusté ESG proposé par \\citet{pedersen2020} :\n\n\\begin{equation}\nSharpe_{ESG} = \\frac{r_p - r_f}{\\sigma_p} + \\lambda \\cdot \\frac{ESG_p - ESG_b}{\\sigma_{ESG}}\n\\end{equation}\n\noù $r_p$ et $\\sigma_p$ sont respectivement le rendement et la volatilité du portefeuille, $r_f$ le taux sans risque, $ESG_p$ et $ESG_b$ les scores ESG du portefeuille et du benchmark, $\\sigma_{ESG}$ l'écart-type des scores ESG dans l'univers d'investissement, et $\\lambda$ un paramètre reflétant l'importance relative accordée à la performance ESG.\n\nCes développements méthodologiques témoignent de l'émergence d'un cadre analytique de plus en plus sophistiqué pour l'intégration quantitative des facteurs ESG dans l'évaluation du risque de crédit, ouvrant la voie à des approches plus robustes et nuancées que les filtres simplistes ou les ajustements qualitatifs ad hoc qui ont longtemps dominé ce domaine.\n\n\\subsection{Interprétabilité des modèles et défis dans leur application en finance}\n\nMalgré leurs performances souvent supérieures, l'adoption des modèles de machine learning dans la gestion du risque de crédit se heurte à des défis significatifs, particulièrement en termes d'interprétabilité. Ce caractère de \"boîte noire\" constitue un obstacle majeur dans un domaine où la compréhension des facteurs de risque et la justification des décisions revêtent une importance cruciale, tant du point de vue réglementaire que de la gouvernance interne.\n\nComme le souligne \\citet{rudin2019} dans son article fondamental \"Stop Explaining Black Box Models for High-Stakes Decisions and Use Interpretable Models Instead\", l'utilisation de modèles opaques pour des décisions à fort impact comme l'octroi de crédit soulève des questions éthiques et pratiques majeures. Elle distingue deux approches de l'interprétabilité : l'utilisation de modèles intrinsèquement interprétables (comme les arbres de décision simples ou les modèles additifs généralisés), et l'application post-hoc de techniques d'explication à des modèles complexes.\n\nDans la seconde catégorie, la méthode SHAP (SHapley Additive exPlanations), développée par \\citet{lundberg2017}, a émergé comme l'une des techniques les plus robustes pour expliquer les prédictions des modèles de machine learning. Basée sur la théorie des jeux coopératifs, SHAP attribue à chaque variable une valeur d'importance qui représente sa contribution marginale à la prédiction, moyennée sur toutes les combinaisons possibles d'autres variables :\n\n\\begin{equation}\n\\phi_i(f, x) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} [f_x(S \\cup \\{i\\}) - f_x(S)]\n\\end{equation}\n\noù $\\phi_i$ représente la valeur SHAP pour la caractéristique i, N l'ensemble de toutes les caractéristiques, S un sous-ensemble de caractéristiques, et $f_x$ la fonction de prédiction conditionnée sur les valeurs observées x.\n\nDans le contexte spécifique du risque de crédit, \\citet{bracke2019} ont appliqué cette méthode pour expliquer les prédictions de modèles XGBoost sur des portefeuilles de prêts hypothécaires. Leur étude a démontré comment SHAP permettait non seulement de quantifier l'importance relative des différentes variables (ratio prêt-valeur, scores de crédit, etc.), mais aussi d'analyser comment leur influence variait selon les segments d'emprunteurs, offrant ainsi des insights précieux pour la gestion différenciée du risque.\n\nUne approche alternative pour améliorer l'interprétabilité consiste à contraindre la structure même des modèles. Les modèles additifs généralisés (GAM), popularisés en finance par \\citet{caruana2015}, décomposent la fonction de prédiction en une somme de fonctions univariées, facilitant ainsi la visualisation et l'interprétation de l'effet marginal de chaque variable :\n\n\\begin{equation}\ng(E[y]) = \\beta_0 + \\sum_{j=1}^p f_j(x_j)\n\\end{equation}\n\noù g est une fonction de lien, $f_j$ des fonctions non paramétriques estimées à partir des données, et $x_j$ les variables explicatives.\n\nDans leur application à la modélisation du risque de crédit, \\citet{dumitrescu2022} ont démontré que ces modèles pouvaient atteindre des performances comparables à celles des algorithmes de machine learning les plus avancés, tout en offrant une interprétabilité nettement supérieure. Leur étude a notamment mis en évidence comment les GAM permettaient d'identifier des relations non linéaires importantes entre certains ratios financiers et le risque de défaut, relations que les modèles logistiques traditionnels ne capturaient pas.\n\nAu-delà de l'interprétabilité, l'application des modèles de machine learning en finance soulève d'autres défis significatifs. La stabilité temporelle de ces modèles, notamment, constitue une préoccupation majeure dans un environnement caractérisé par des changements de régime et des crises sporadiques. \\citet{moscatelli2020} ont étudié cette question en comparant la robustesse de différents algorithmes face à des perturbations structurelles simulées et réelles. Leurs résultats suggèrent que les modèles de gradient boosting, comme XGBoost, présentent généralement une meilleure stabilité que les réseaux de neurones profonds, mais restent néanmoins vulnérables aux changements de régime majeurs, soulignant l'importance des tests de stress et des analyses de sensibilité dans l'évaluation de ces modèles.\n\nLe problème du déséquilibre des classes, particulièrement prononcé dans les données de crédit où les défauts sont rares, constitue un autre défi méthodologique majeur. \\citet{fernandez2018} ont réalisé une revue exhaustive des techniques pour traiter ce problème, incluant le rééchantillonnage (sur-échantillonnage de la classe minoritaire ou sous-échantillonnage de la classe majoritaire), la génération synthétique d'exemples minoritaires (SMOTE), et l'ajustement des fonctions de coût. Leur analyse suggère qu'aucune approche n'est universellement supérieure, et que la combinaison de plusieurs techniques, adaptées aux caractéristiques spécifiques du problème, offre généralement les meilleurs résultats.\n\nDu point de vue réglementaire, l'adoption des modèles de machine learning pour l'évaluation du risque de crédit soulève des questions importantes concernant leur validation et leur surveillance. Le \\citet{comite2018} a publié des recommandations spécifiques à ce sujet, soulignant notamment l'importance d'une gouvernance appropriée des modèles, d'une validation rigoureuse intégrant des tests de robustesse, et d'une surveillance continue de leur performance. Ces exigences réglementaires peuvent constituer un frein à l'adoption de modèles trop complexes ou insuffisamment interprétables.\n\nDans une perspective plus large, l'intégration des critères ESG dans les modèles de machine learning pour le risque de crédit soulève des défis méthodologiques spécifiques. \\citet{bolton2021} ont mis en évidence la complexité de cette intégration, notamment en raison de la non-stationnarité des relations entre facteurs ESG et risque financier (due à l'évolution des réglementations et des attentes sociétales), de l'hétérogénéité sectorielle des impacts ESG, et des interactions complexes entre les différentes dimensions E, S et G. Leur étude suggère l'importance d'approches flexibles, spécifiques aux secteurs, et intégrant explicitement la dimension temporelle pour capturer ces relations dynamiques.\n\n\\section{Conclusion et transition}\n\nCette revue de littérature a permis d'explorer les trois piliers fondamentaux de notre recherche : les modèles traditionnels du risque de crédit, l'intégration des critères ESG dans l'évaluation financière, et l'application des techniques de machine learning à la modélisation du risque. Ces domaines, bien qu'ayant été largement étudiés séparément, présentent d'importantes synergies potentielles encore insuffisamment explorées dans la littérature académique et les pratiques professionnelles.\n\nLes modèles structurels et réduits, bien qu'offrant un cadre théorique rigoureux, peinent à intégrer de manière satisfaisante les facteurs extra-financiers comme les critères ESG. Les approches statistiques traditionnelles, quant à elles, se heurtent à des limitations en termes de capacité à capturer des relations non linéaires complexes. Parallèlement, les avancées en machine learning offrent des perspectives prometteuses pour surmonter ces limitations, mais soulèvent des défis importants en termes d'interprétabilité et de robustesse.\n\nL'influence croissante des critères ESG sur le risque de crédit est aujourd'hui largement reconnue, mais sa formalisation quantitative dans les modèles de risque reste insuffisamment développée. Cette lacune constitue à la fois un défi méthodologique et une opportunité de recherche significative, particulièrement pertinente dans le contexte actuel de transition écologique et de montée des préoccupations sociales et de gouvernance.\n\nDans les chapitres suivants, nous nous appuierons sur cette base théorique pour développer et tester empiriquement une méthodologie intégrant les critères ESG dans la modélisation du risque de crédit d'un portefeuille obligataire, en exploitant le potentiel des techniques de machine learning tout en adressant leurs limitations. Cette approche vise à combler la lacune identifiée dans la littérature et à fournir un cadre analytique robuste pour les praticiens de la gestion des risques et de l'investissement responsable.