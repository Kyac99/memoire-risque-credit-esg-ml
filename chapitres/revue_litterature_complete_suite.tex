\section{L'essor du Machine Learning dans la gestion du risque de crédit}

\subsection{Modèles supervisés : Random Forest, XGBoost, réseaux de neurones}

L'application des techniques de machine learning à la modélisation du risque de crédit a connu un essor considérable ces dernières années, offrant des alternatives aux approches statistiques traditionnelles. Les modèles supervisés, en particulier, ont démontré une capacité remarquable à capturer des relations complexes et non linéaires entre les variables explicatives et le risque de défaut.

Les algorithmes de Random Forest, introduits par \citet{breiman2001}, constituent l'une des approches les plus utilisées dans ce domaine. Ces modèles d'ensemble combinent de multiples arbres de décision entraînés sur des sous-échantillons aléatoires des données et des sous-ensembles aléatoires de variables, réduisant ainsi le risque de surapprentissage. Formellement, la prédiction d'un modèle Random Forest peut être exprimée comme :

\begin{equation}
\hat{y} = \frac{1}{B} \sum_{b=1}^{B} T_b(X)
\end{equation}

où B est le nombre d'arbres, $T_b$ représente le b-ième arbre, et X le vecteur de variables explicatives.

\citet{brown2012} ont comparé les performances de différentes techniques de machine learning, dont Random Forest, pour la prédiction du risque de crédit sur des ensembles de données déséquilibrés, caractéristiques des portefeuilles de crédit où les défauts sont rares. Leur étude a démontré que Random Forest surpassait systématiquement les méthodes statistiques traditionnelles, notamment les modèles logit et probit, ainsi que d'autres algorithmes de machine learning comme les SVM (Support Vector Machines), particulièrement dans les cas d'asymétrie prononcée entre les classes.

XGBoost (eXtreme Gradient Boosting), développé par \citet{chen2016}, représente une évolution majeure des algorithmes de boosting pour les arbres de décision. Contrairement à Random Forest qui construit des arbres indépendants en parallèle, XGBoost adopte une approche séquentielle où chaque nouvel arbre est optimisé pour corriger les erreurs des arbres précédents. La fonction objective de XGBoost combine un terme de perte et un terme de régularisation :

\begin{equation}
\mathcal{L} = \sum_{i=1}^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)
\end{equation}

où $l$ est la fonction de perte, $\hat{y}_i$ la prédiction pour l'observation i, $\Omega$ le terme de régularisation, et $f_k$ représente le k-ième arbre du modèle.

Dans une étude comparative extensive, \citet{xia2017} ont évalué la performance de XGBoost contre plusieurs autres techniques pour la prédiction du risque de crédit sur 10 ensembles de données de référence. Leurs résultats ont révélé que XGBoost surpassait systématiquement les autres approches en termes de précision et d'aire sous la courbe ROC (AUC), tout en offrant une meilleure stabilité face aux variations dans les données d'entraînement.

Les réseaux de neurones artificiels (ANN) constituent une troisième catégorie majeure de modèles supervisés appliqués au risque de crédit. Dans leur forme la plus simple, les réseaux de neurones multicouches transforment un vecteur d'entrée X en une prédiction y à travers une série de transformations non linéaires :

\begin{align}
h^{(1)} &= \sigma(W^{(1)}X + b^{(1)}) \\
h^{(2)} &= \sigma(W^{(2)}h^{(1)} + b^{(2)}) \\
\vdots \\
y &= \sigma(W^{(L)}h^{(L-1)} + b^{(L)})
\end{align}

où $h^{(l)}$ représente la l-ième couche cachée, $W^{(l)}$ et $b^{(l)}$ les paramètres associés, et $\sigma$ une fonction d'activation non linéaire comme la fonction ReLU (Rectified Linear Unit) : $\sigma(x) = max(0, x)$.

\citet{kvamme2018} ont proposé une architecture de réseau neuronal profond spécifiquement conçue pour la modélisation du risque de crédit, intégrant non seulement la prédiction de la probabilité de défaut mais aussi l'estimation de la distribution temporelle des défauts. Leur modèle, basé sur des réseaux de neurones récurrents avec portes (LSTM - Long Short-Term Memory), a démontré une capacité supérieure à capturer les dépendances temporelles dans les données de crédit par rapport aux approches traditionnelles de modélisation de survie.

\begin{align}
h_t &= LSTM(x_t, h_{t-1})\\
S(t) &= \sigma(W_s h_t + b_s)
\end{align}

où $S(t)$ représente la fonction de survie estimée, indiquant la probabilité de non-défaut jusqu'au temps t.

Plus récemment, \citet{babaev2019} ont exploré l'utilisation de techniques d'apprentissage profond avancées, notamment les modèles d'attention, pour la prédiction du risque de crédit à partir de séquences d'événements transactionnels. Leur approche permet de pondérer dynamiquement l'importance de différentes transactions dans l'historique d'un client, offrant ainsi une capacité de modélisation plus fine des comportements risqués :

\begin{align}
\alpha_i &= \frac{\exp(e_i^T W_{\alpha} h)}{\sum_j \exp(e_j^T W_{\alpha} h)}\\
c &= \sum_i \alpha_i e_i
\end{align}

où $\alpha_i$ représente le poids d'attention attribué à l'événement i, $e_i$ son encodage vectoriel, et c le vecteur de contexte résultant, utilisé pour la prédiction finale.

Dans une perspective comparative globale, \citet{lessmann2015} ont conduit une analyse exhaustive des méthodes de machine learning pour le scoring de crédit, évaluant 41 classifieurs différents sur plusieurs ensembles de données de référence. Leurs résultats ont confirmé la supériorité des méthodes d'ensemble comme Random Forest et Gradient Boosting, tout en soulignant que la combinaison de plusieurs modèles hétérogènes (stacking) pouvait conduire à des performances encore supérieures. Ils ont également mis en évidence l'importance d'une évaluation appropriée des modèles, recommandant l'utilisation de métriques adaptées aux problématiques de crédit comme le H-measure et l'AUC ajustée aux coûts (Cost-AUC).

\subsection{Modèles non supervisés : clustering et réduction de dimension}

Les techniques d'apprentissage non supervisé, bien que moins directement appliquées à la prédiction du risque de crédit que les modèles supervisés, jouent néanmoins un rôle important dans la préparation des données, la détection d'anomalies et la segmentation des portefeuilles. Ces approches permettent d'extraire des structures et des patterns inhérents aux données sans nécessiter d'étiquettes préalables.

Les algorithmes de clustering visent à regrouper les observations similaires, permettant ainsi d'identifier des segments homogènes au sein d'un portefeuille de crédit. Parmi ces techniques, l'algorithme K-means demeure l'un des plus utilisés. Il cherche à minimiser la variance intra-cluster en partitionnant les données en K groupes :

\begin{equation}
\min_{C} \sum_{k=1}^K \sum_{i \in C_k} ||x_i - \mu_k||^2
\end{equation}

où $C_k$ représente le k-ième cluster, $x_i$ les observations, et $\mu_k$ le centroïde du cluster k.

\citet{harris2015} a appliqué cette approche à la segmentation des emprunteurs obligataires, démontrant que l'identification de segments homogènes permettait d'améliorer significativement la précision des modèles de risque spécifiques à chaque segment par rapport à un modèle global. Son étude a également souligné l'importance d'une sélection appropriée du nombre de clusters, suggérant l'utilisation de critères comme l'indice de silhouette ou la méthode du coude (elbow method).

Des approches plus sophistiquées comme le clustering hiérarchique agglomératif offrent une flexibilité supplémentaire en construisant une hiérarchie de clusters sans nécessiter la spécification préalable de leur nombre. Comme le décrit \citet{aggarwal2014}, cette méthode procède par fusion successive des clusters les plus similaires, jusqu'à n'en former plus qu'un seul :

\begin{align}
d(C_i, C_j) &= \min_{x \in C_i, y \in C_j} d(x, y) \quad \text{(single linkage)}\\
d(C_i, C_j) &= \max_{x \in C_i, y \in C_j} d(x, y) \quad \text{(complete linkage)}\\
d(C_i, C_j) &= \frac{1}{|C_i||C_j|} \sum_{x \in C_i} \sum_{y \in C_j} d(x, y) \quad \text{(average linkage)}
\end{align}

où $d(C_i, C_j)$ représente la distance entre les clusters $C_i$ et $C_j$, calculée selon différentes définitions (single, complete ou average linkage).

Dans le domaine spécifique du risque de crédit obligataire, \citet{poon2019} ont appliqué ces techniques à l'identification de groupes d'émetteurs présentant des profils de risque similaires. Leur approche, combinant clustering hiérarchique et analyse en composantes principales, a permis de révéler des structures de risque latentes non apparentes dans les classifications sectorielles traditionnelles, offrant ainsi des opportunités de diversification plus efficaces pour les gestionnaires de portefeuilles obligataires.

Les méthodes de réduction de dimension constituent une autre catégorie importante d'approches non supervisées, particulièrement utiles pour traiter la haute dimensionnalité des données financières et ESG. L'Analyse en Composantes Principales (ACP), introduite par \citet{pearson1901} et développée par \citet{hotelling1933}, reste l'une des techniques les plus utilisées. Elle vise à projeter les données dans un espace de dimension réduite tout en maximisant la variance expliquée :

\begin{equation}
\max_{w} \frac{w^T X^T X w}{w^T w}
\end{equation}

sous contrainte $w^T w = 1$, où X représente la matrice de données centrées, et w les vecteurs propres de la matrice de covariance $X^T X$.

Appliquée aux critères ESG, l'ACP permet de réduire la redondance entre les multiples indicateurs et d'identifier les dimensions latentes principales. \citet{dorfleitner2018} ont utilisé cette approche pour analyser la structure sous-jacente des notations ESG, démontrant que malgré la multiplicité des indicateurs, un nombre relativement restreint de composantes principales (généralement 3 à 5) capturait la majeure partie de la variance. Cette observation suggère une dimensionnalité intrinsèque relativement faible des données ESG, facilitant leur intégration dans les modèles de risque de crédit.

Des méthodes non linéaires de réduction de dimension, comme t-SNE (t-distributed Stochastic Neighbor Embedding) développée par \citet{vandermaaten2008}, offrent des perspectives complémentaires en préservant les structures locales des données. Contrairement à l'ACP qui se concentre sur la variance globale, t-SNE minimise la divergence de Kullback-Leibler entre les distributions de probabilité dans l'espace original et l'espace réduit :

\begin{equation}
KL(P || Q) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}
\end{equation}

où $p_{ij}$ et $q_{ij}$ représentent respectivement les similitudes entre les points i et j dans l'espace original et l'espace réduit.

Appliquée aux données de crédit, cette approche permet de visualiser des clusters d'émetteurs similaires qui pourraient ne pas être apparents avec des méthodes linéaires. Comme le démontrent \citet{greco2019}, cette visualisation peut être particulièrement utile pour identifier des anomalies ou des tendances émergentes dans les profils de risque, complétant ainsi les modèles prédictifs supervisés.

Plus récemment, les autoencodeurs, une classe de réseaux de neurones conçus pour l'apprentissage de représentations efficientes, ont été appliqués avec succès à la réduction de dimension en finance. Un autoencodeur consiste en un encodeur qui compresse les données d'entrée en une représentation latente de dimension réduite, et un décodeur qui tente de reconstruire les données originales à partir de cette représentation :

\begin{align}
h &= f_{\theta}(x) \quad \text{(encodage)}\\
\hat{x} &= g_{\phi}(h) \quad \text{(décodage)}
\end{align}

L'objectif est de minimiser l'erreur de reconstruction : $L(x, \hat{x}) = ||x - \hat{x}||^2$, tout en contraignant la dimension de h à être significativement inférieure à celle de x.

Dans leur étude, \citet{petropoulos2019} ont appliqué des autoencodeurs variationnels à la modélisation du risque de crédit, démontrant leur capacité à extraire des représentations latentes informatives à partir de données financières et extra-financières hétérogènes. Ces représentations ont ensuite été utilisées comme entrées pour des modèles prédictifs supervisés, améliorant significativement leur performance par rapport à l'utilisation directe des variables brutes.

\subsection{Interprétabilité des modèles et défis dans leur application en finance}

Malgré leurs performances souvent supérieures, l'adoption des modèles de machine learning dans la gestion du risque de crédit se heurte à des défis significatifs, particulièrement en termes d'interprétabilité. Ce caractère de "boîte noire" constitue un obstacle majeur dans un domaine où la compréhension des facteurs de risque et la justification des décisions revêtent une importance cruciale, tant du point de vue réglementaire que de la gouvernance interne.

Comme le souligne \citet{rudin2019} dans son article fondamental "Stop Explaining Black Box Models for High-Stakes Decisions and Use Interpretable Models Instead", l'utilisation de modèles opaques pour des décisions à fort impact comme l'octroi de crédit soulève des questions éthiques et pratiques majeures. Elle distingue deux approches de l'interprétabilité : l'utilisation de modèles intrinsèquement interprétables (comme les arbres de décision simples ou les modèles additifs généralisés), et l'application post-hoc de techniques d'explication à des modèles complexes.

Dans la seconde catégorie, la méthode SHAP (SHapley Additive exPlanations), développée par \citet{lundberg2017}, a émergé comme l'une des techniques les plus robustes pour expliquer les prédictions des modèles de machine learning. Basée sur la théorie des jeux coopératifs, SHAP attribue à chaque variable une valeur d'importance qui représente sa contribution marginale à la prédiction, moyennée sur toutes les combinaisons possibles d'autres variables :

\begin{equation}
\phi_i(f, x) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} [f_x(S \cup \{i\}) - f_x(S)]
\end{equation}

où $\phi_i$ représente la valeur SHAP pour la caractéristique i, N l'ensemble de toutes les caractéristiques, S un sous-ensemble de caractéristiques, et $f_x$ la fonction de prédiction conditionnée sur les valeurs observées x.

Dans le contexte spécifique du risque de crédit, \citet{bracke2019} ont appliqué cette méthode pour expliquer les prédictions de modèles XGBoost sur des portefeuilles de prêts hypothécaires. Leur étude a démontré comment SHAP permettait non seulement de quantifier l'importance relative des différentes variables (ratio prêt-valeur, scores de crédit, etc.), mais aussi d'analyser comment leur influence variait selon les segments d'emprunteurs, offrant ainsi des insights précieux pour la gestion différenciée du risque.

Une approche alternative pour améliorer l'interprétabilité consiste à contraindre la structure même des modèles. Les modèles additifs généralisés (GAM), popularisés en finance par \citet{caruana2015}, décomposent la fonction de prédiction en une somme de fonctions univariées, facilitant ainsi la visualisation et l'interprétation de l'effet marginal de chaque variable :

\begin{equation}
g(E[y]) = \beta_0 + \sum_{j=1}^p f_j(x_j)
\end{equation}

où g est une fonction de lien, $f_j$ des fonctions non paramétriques estimées à partir des données, et $x_j$ les variables explicatives.

Dans leur application à la modélisation du risque de crédit, \citet{dumitrescu2022} ont démontré que ces modèles pouvaient atteindre des performances comparables à celles des algorithmes de machine learning les plus avancés, tout en offrant une interprétabilité nettement supérieure. Leur étude a notamment mis en évidence comment les GAM permettaient d'identifier des relations non linéaires importantes entre certains ratios financiers et le risque de défaut, relations que les modèles logistiques traditionnels ne capturaient pas.

Au-delà de l'interprétabilité, l'application des modèles de machine learning en finance soulève d'autres défis significatifs. La stabilité temporelle de ces modèles, notamment, constitue une préoccupation majeure dans un environnement caractérisé par des changements de régime et des crises sporadiques. \citet{moscatelli2020} ont étudié cette question en comparant la robustesse de différents algorithmes face à des perturbations structurelles simulées et réelles. Leurs résultats suggèrent que les modèles de gradient boosting, comme XGBoost, présentent généralement une meilleure stabilité que les réseaux de neurones profonds, mais restent néanmoins vulnérables aux changements de régime majeurs, soulignant l'importance des tests de stress et des analyses de sensibilité dans l'évaluation de ces modèles.

Le problème du déséquilibre des classes, particulièrement prononcé dans les données de crédit où les défauts sont rares, constitue un autre défi méthodologique majeur. \citet{fernandez2018} ont réalisé une revue exhaustive des techniques pour traiter ce problème, incluant le rééchantillonnage (sur-échantillonnage de la classe minoritaire ou sous-échantillonnage de la classe majoritaire), la génération synthétique d'exemples minoritaires (SMOTE), et l'ajustement des fonctions de coût. Leur analyse suggère qu'aucune approche n'est universellement supérieure, et que la combinaison de plusieurs techniques, adaptées aux caractéristiques spécifiques du problème, offre généralement les meilleurs résultats.

Du point de vue réglementaire, l'adoption des modèles de machine learning pour l'évaluation du risque de crédit soulève des questions importantes concernant leur validation et leur surveillance. Le \citet{comite2018} a publié des recommandations spécifiques à ce sujet, soulignant notamment l'importance d'une gouvernance appropriée des modèles, d'une validation rigoureuse intégrant des tests de robustesse, et d'une surveillance continue de leur performance. Ces exigences réglementaires peuvent constituer un frein à l'adoption de modèles trop complexes ou insuffisamment interprétables.

Dans une perspective plus large, l'intégration des critères ESG dans les modèles de machine learning pour le risque de crédit soulève des défis méthodologiques spécifiques. \citet{bolton2021} ont mis en évidence la complexité de cette intégration, notamment en raison de la non-stationnarité des relations entre facteurs ESG et risque financier (due à l'évolution des réglementations et des attentes sociétales), de l'hétérogénéité sectorielle des impacts ESG, et des interactions complexes entre les différentes dimensions E, S et G. Leur étude suggère l'importance d'approches flexibles, spécifiques aux secteurs, et intégrant explicitement la dimension temporelle pour capturer ces relations dynamiques.

\section{Conclusion et transition}

Cette revue de littérature a permis d'explorer les trois piliers fondamentaux de notre recherche : les modèles traditionnels du risque de crédit, l'intégration des critères ESG dans l'évaluation financière, et l'application des techniques de machine learning à la modélisation du risque. Ces domaines, bien qu'ayant été largement étudiés séparément, présentent d'importantes synergies potentielles encore insuffisamment explorées dans la littérature académique et les pratiques professionnelles.

Les modèles structurels et réduits, bien qu'offrant un cadre théorique rigoureux, peinent à intégrer de manière satisfaisante les facteurs extra-financiers comme les critères ESG. Les approches statistiques traditionnelles, quant à elles, se heurtent à des limitations en termes de capacité à capturer des relations non linéaires complexes. Parallèlement, les avancées en machine learning offrent des perspectives prometteuses pour surmonter ces limitations, mais soulèvent des défis importants en termes d'interprétabilité et de robustesse.

L'influence croissante des critères ESG sur le risque de crédit est aujourd'hui largement reconnue, mais sa formalisation quantitative dans les modèles de risque reste insuffisamment développée. Cette lacune constitue à la fois un défi méthodologique et une opportunité de recherche significative, particulièrement pertinente dans le contexte actuel de transition écologique et de montée des préoccupations sociales et de gouvernance.

Dans les chapitres suivants, nous nous appuierons sur cette base théorique pour développer et tester empiriquement une méthodologie intégrant les critères ESG dans la modélisation du risque de crédit d'un portefeuille obligataire, en exploitant le potentiel des techniques de machine learning tout en adressant leurs limitations. Cette approche vise à combler la lacune identifiée dans la littérature et à fournir un cadre analytique robuste pour les praticiens de la gestion des risques et de l'investissement responsable.